{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 직접 해보기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 모듈 import\n",
    "############################\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 사전 클래스 정의\n",
    "############################\n",
    "class GerEngVocab():\n",
    "    def __init__(self, datas, max_len):\n",
    "        \n",
    "        data_iter = self.__iter_vocab(datas)\n",
    "        \n",
    "        sep_list = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "        \n",
    "        self.vocab = build_vocab_from_iterator(\n",
    "            data_iter,\n",
    "            specials=sep_list\n",
    "        )\n",
    "        \n",
    "        self.vocab.set_default_index(self.vocab['<unk>'])\n",
    "        \n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.idx2word = self.vocab.get_itos()\n",
    "        self.word2idx = self.vocab.get_stoi()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "    \n",
    "    # 단어 -> 인덱스\n",
    "    def wrd2idx(self, word):\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    # 인덱스 -> 단어\n",
    "    def idx2wrd(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "    \n",
    "    # vocab 생성을 위한 iterator\n",
    "    def __iter_vocab(datas):\n",
    "        for data in tqdm(datas):\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 데이터셋 클래스 정의\n",
    "############################\n",
    "class GerEngDataset(Dataset):\n",
    "    ########################\n",
    "    # 필수 함수 정의\n",
    "    ########################\n",
    "    def __init__(self, vocab, X, y=[], max_len=10, emb_type='ohe', train_mode=True):\n",
    "        \n",
    "        self.__train_mode = train_mode if y == [] else False\n",
    "        self.__emb_type = emb_type\n",
    "        self.__max_len = max_len\n",
    "        self.__vocab = GerEngVocab(X+y, max_len)\n",
    "        self.__oh_vector = torch.eye(self.__vocab)\n",
    "        self.__X = self.__tokenizer(X)\n",
    "        \n",
    "        if y != []:\n",
    "            _y = y\n",
    "            self.__y = self.__tokenizer(y)\n",
    "            self.__dec_input = self.__add_sep_token(self.__y)\n",
    "            self.__target = self.__add_sep_token(self.__y, sep_token='<eos>')\n",
    "            self.__fake_dec_input = self.__add_sep_token(self.__get_initial_dec_input())\n",
    "        else:\n",
    "            self.__y = None\n",
    "            self.__fake_dec_input = self.__add_sep_token(self.__get_initial_dec_input())\n",
    "            pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.__X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.__emb_type == 'ohe':\n",
    "            return self.__ohe_getitem(idx)\n",
    "        elif self.__emb_type == 'emb':\n",
    "            return self.__emb_getitem(idx)\n",
    "        \n",
    "    ########################\n",
    "    # 기타 함수 정의\n",
    "    ########################\n",
    "    def __ohe_getitem(self, idx):\n",
    "        ohe_X = self.__oh_encoding(self.__X[idx])\n",
    "        if self.__train_mode and self.__y is not None:\n",
    "            ohe_dec_input = self.__oh_encoding(self.__dec_input[idx])\n",
    "            ohe_target = self.__oh_encoding(self.__target[idx])\n",
    "            return torch.tensor(ohe_X), torch.tensor(ohe_dec_input), torch.tensor(ohe_target)\n",
    "        else:\n",
    "            ohe_dec_input = self.__oh_encoding(self.__fake_dec_input[idx])\n",
    "            return torch.tensor(ohe_X), torch.tensor(ohe_dec_input), torch.tensor(ohe_dec_input)\n",
    "    \n",
    "    def __emb_getitem(self, idx):\n",
    "        if self.__train_mode and self.__y is not None:\n",
    "            return self.__X[idx], self.__dec_input[idx], self.__target[idx]\n",
    "        else:\n",
    "            return self.__X[idx], self.__fake_dec_input[idx], self.__fake_dec_input[idx].copy()\n",
    "    \n",
    "    def __tokenizer(self, datas):\n",
    "        padding_datas = self.__padding(datas)\n",
    "        for data in padding_datas:\n",
    "            data = [self.__vocab.wrd2idx(wrd) for wrd in data]\n",
    "        return np.array(padding_datas)\n",
    "    \n",
    "    def __padding(self, datas):\n",
    "        for data in datas:\n",
    "            data = data + ['<pad>' for _ in range(self.__max_len-len(data))]\n",
    "        return datas\n",
    "    \n",
    "    def __oh_encoding(self, datas):\n",
    "        return self.__oh_vector[self.__vocab.wrd2idx(datas)]\n",
    "    \n",
    "    def __add_sep_token(self, token_datas, sep_token='<sos>'):\n",
    "        added_array = None\n",
    "        _array = np.array([self.__vocab.wrd2idx(sep_token) for _ in range(len(self.__X))])\n",
    "        if sep_token == '<sos>':\n",
    "            added_array = np.concatenate((_array.reshape(-1, 1), token_datas), axis=1)\n",
    "        elif sep_token == '<eos>':\n",
    "            added_array = np.concatenate((token_datas, _array.reshape(-1, 1)), axis=1)\n",
    "        return added_array\n",
    "    \n",
    "    def __get_initial_dec_input(self):\n",
    "        fake_dec_input = []\n",
    "        for _ in range(len(self.__X)):\n",
    "            fake_dec_input.append([self.__vocab.wrd2idx('<pad>') for _ in range(self.__max_len)])\n",
    "        return np.array(fake_dec_input)\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.__vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 레이어 클래스 정의\n",
    "############################\n",
    "# 임베딩 레이어\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.__emb = nn.Embedding(vocab_size, emb_dim)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.__emb(X)\n",
    "\n",
    "# Attention 레이어\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        self.__hidden_size = hidden_size\n",
    "        self.__shape_size = (2 if bidirectional else 1) * n_layers\n",
    "        \n",
    "        self.__encoding_layer = nn.LSTM(\n",
    "            input_size=input_size,      # 입력값의 차원\n",
    "            hidden_size=hidden_size,    # 히든레이어의 차원\n",
    "            num_layers=n_layers,        # 각 히든당 레이어의 갯수\n",
    "            bidirectional=bidirectional,# 양방향 설정\n",
    "        )\n",
    "        self.__decoding_layer = nn.LSTM(\n",
    "            input_size=input_size,      # 입력값의 차원\n",
    "            hidden_size=hidden_size,    # 히든레이어의 차원\n",
    "            num_layers=n_layers,        # 각 히든당 레이어의 갯수\n",
    "            bidirectional=bidirectional,# 양방향 설정\n",
    "        )\n",
    "    \n",
    "    def forward(self, enc_input, dec_input):\n",
    "        # 초기값 설정\n",
    "        initial_hidden = torch.zeros(self.__shape_size, enc_input.size(0), self.__hidden_size).to(enc_input.device)\n",
    "        initial_cell = torch.zeros(self.__shape_size, enc_input.size(0), self.__hidden_size).to(enc_input.device)\n",
    "        # 인코딩\n",
    "        enc_output, (enc_hidden, enc_cell) = self.__encoding_layer(enc_input, (initial_hidden, initial_cell))\n",
    "        n_step = dec_input.size(0)\n",
    "        hidden_state, cell_state = enc_hidden, enc_cell\n",
    "        # 스텝의 attention 가중치를 저장할 리스트\n",
    "        train_attn_weights = []\n",
    "        # 결과값을 저장할 텐서\n",
    "        responses = torch.empty(n_step, enc_input.size(0), self.__hidden_size*2).to(enc_input.device)\n",
    "        # n_step 별 디코딩\n",
    "        for i in range(n_step):\n",
    "            # 현재 스텝의 디코딩\n",
    "            dec_output, (hidden_state, cell_state) = self.__decoding_layer(dec_input[i].unsqueeze(0), (hidden_state, cell_state))\n",
    "            # 현재 스텝의 가중치 계산\n",
    "            # weight.shape (batch_size, 1, n_step)\n",
    "            weight = self.__get_attention_weight(enc_output, dec_output)\n",
    "            # 현재 스텝의 가중치를 저장\n",
    "            train_attn_weights.append(weight.squeeze().data.cpu().numpy())\n",
    "            \n",
    "            # weight를 이용해 attention value 계산\n",
    "            # A.shape (a, b, C)\n",
    "            # B.shape (a, C, d)\n",
    "            # A.bmm(B).shape -> (a, b, d) (bmm = 배치별 행렬곱)\n",
    "            \n",
    "            # weight.shape                     : (batch_size, 1, n_step) \n",
    "            # enc_output.transpose(0, 1).shape : (batch_size, n_step, n_hidden)\n",
    "            # attn_value.shape (batch_size, 1, n_hidden)\n",
    "            attn_value = weight.bmm(enc_output.transpose(0, 1))\n",
    "            \n",
    "            # dec_output과 attn_value의 shape을 맞춰줌\n",
    "            dec_output = dec_output.squeeze(0) # (, batch_size, n_hidden) -> (batch_size, n_hidden)\n",
    "            attn_value = attn_value.squeeze(1) # (batch_size, 1, n_hidden) -> (batch_size, n_hidden)\n",
    "            # dec_output과 attn_value를 연결후 reponses에 저장\n",
    "            responses[i] = torch.cat((dec_output, attn_value), dim=-1)\n",
    "            \n",
    "        # responses.shape (n_step, batch_size, n_hidden*2)\n",
    "        return responses, train_attn_weights\n",
    "    \n",
    "    def __get_attention_weight(self, enc_output, dec_output):\n",
    "        n_step = enc_output.size(0)\n",
    "        batch_size = enc_output.size(1)\n",
    "        # 값을 저장할 텐서 생성\n",
    "        # attention_score.shape (n_step, batch_size)\n",
    "        attention_score = torch.zeros((batch_size, n_step)).to(enc_output.device)\n",
    "        # 각 스텝별 attention score 계산\n",
    "        for i in range(n_step):\n",
    "        # 내적을 이용해 attention score 계산\n",
    "        # enc_output[i].shape (batch_size, n_hidden)\n",
    "        # dec_output.shape    (batch_size, n_hidden)\n",
    "            for j in range(batch_size):\n",
    "                attention_score[j, i] = torch.dot(enc_output[i, j], dec_output[j])\n",
    "        # attention score를 softmax를 이용해 확률값으로 변환\n",
    "        # attention_score.shape (batch_size, n_step)\n",
    "        # attention_score.unsqueeze(1).shape (batch_size, 1, n_step)\n",
    "        return F.softmax(attention_score, dim=-1).unsqueeze(1)\n",
    "    \n",
    "# 아웃풋 레이어\n",
    "class OutputLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.__linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X.shape (n_step, batch_size, n_hidden*2)\n",
    "        n_step  = X.size(0)\n",
    "        _X = X\n",
    "        for i in range(n_step):\n",
    "            # X[i].shape (batch_size, n_hidden*2)\n",
    "            # _X[i].shape (batch_size, output_size)\n",
    "            _X[i] = self.__linear(X[i])\n",
    "        # _X.shape (n_step, batch_size, output_size)\n",
    "        # _X.transpose(0, 1).shape (batch_size, n_step, output_size)\n",
    "        return _X.transpose(0, 1)\n",
    "        \n",
    "############################\n",
    "# 모델 클래스 정의\n",
    "############################\n",
    "class Ger2EngAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers, bidirectional, emb_type, cell_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.__emb_type = emb_type\n",
    "        self.__cell_type = cell_type\n",
    "        \n",
    "        # 임베딩 레이어\n",
    "        if self.__emb_type == 'emb':\n",
    "            self.__embedding = nn.Embedding(\n",
    "                input_size=vocab_size,\n",
    "                embedding_dim=emb_dim,\n",
    "            )\n",
    "        # Attention 레이어\n",
    "        if self.__cell_type == 'lstm':\n",
    "            self.__attention = AttentionLSTM(\n",
    "                input_size=emb_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                n_layers=num_layers,\n",
    "                bidirectional=bidirectional,\n",
    "            )\n",
    "        elif self.__cell_type == 'rnn':\n",
    "            pass\n",
    "        # 아웃풋 레이어\n",
    "        self.__output = OutputLinear(\n",
    "            input_size=hidden_dim*2,\n",
    "            output_size=vocab_size,\n",
    "        )\n",
    "    \n",
    "    def forward(self, enc_input, dec_input):\n",
    "        # 임베딩 레이어\n",
    "        if self.__emb_type == 'emb':\n",
    "            enc_input = self.__embedding(enc_input)\n",
    "            dec_input = self.__embedding(dec_input)\n",
    "        # Attention 레이어\n",
    "        dec_output, train_attn_weights = self.__attention(enc_input, dec_input)\n",
    "        # 아웃풋 레이어\n",
    "        output = self.__output(dec_output)\n",
    "        return output, train_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# 얼리스탑핑 정의\n",
    "######################\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience=10, save_path=None, target_score=0, model_name='model'):\n",
    "        # 초기화\n",
    "        self.best_score = 0\n",
    "        self.patience_count = 0\n",
    "        self.target_score = target_score\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        best_model_name = model_name + '_best.pth'\n",
    "        self.best_model_path = self.save_path + best_model_name\n",
    "        last_model_name = model_name + '_last.pth'\n",
    "        self.last_model_path = self.save_path + last_model_name\n",
    "    # 얼리 스토핑 여부 확인 함수 정의\n",
    "    def is_stop(self, model, score):\n",
    "        # 모델 저장(마지막 모델)\n",
    "        self.__save_last_model(model)\n",
    "        # 베스트 스코어가 타겟 스코어보다 낮을 경우\n",
    "        if self.best_score < self.target_score:\n",
    "            # 스코어가 이전보다 안좋을 경우\n",
    "            if self.best_score >= score:\n",
    "                # patience 초기화\n",
    "                self.patience_count = 0\n",
    "                return False\n",
    "            # 스코어를 업데이트\n",
    "            self.best_score = score\n",
    "            # 모델 저장\n",
    "            self.__save_best_model(model)\n",
    "            # patience 초기화\n",
    "            self.patience_count = 0\n",
    "            return False\n",
    "            \n",
    "        \n",
    "        # 스코어가 이전보다 좋을 경우\n",
    "        if self.best_score < score:\n",
    "            # 스코어를 업데이트\n",
    "            self.best_score = score\n",
    "            # 모델 저장\n",
    "            self.__save_best_model(model)\n",
    "            # patience 초기화\n",
    "            self.patience_count = 0\n",
    "            return False\n",
    "        \n",
    "        # 스코어가 이전보다 좋지 않을 경우 +\n",
    "        # 스코어가 타겟 스코어보다 높을 경우\n",
    "        # patience 증가\n",
    "        self.patience_count += 1\n",
    "        # patience가 최대치를 넘을 경우\n",
    "        if self.patience_count > self.patience:\n",
    "            return True\n",
    "        # patience가 최대치를 넘지 않을 경우\n",
    "        return False\n",
    "    # 모델 저장 함수 정의\n",
    "    def __save_best_model(self, model):\n",
    "        torch.save(model.state_dict(), self.best_model_path)\n",
    "    # 마지막 모델 저장 함수 정의\n",
    "    def __save_last_model(self, model):\n",
    "        torch.save(model.state_dict(), self.last_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 모델 학습 함수 정의\n",
    "############################\n",
    "def train(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = None\n",
    "    train_score = None\n",
    "    for enc_input, dec_input, targets in loader:\n",
    "        enc_input = enc_input.to(device)\n",
    "        dec_input = dec_input.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs, _ = model(enc_input, dec_input)\n",
    "        loss = None\n",
    "        for output, target in zip(outputs, targets):\n",
    "            loss = loss_fn(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = loss.item()\n",
    "        \n",
    "        outputs = F.softmax(outputs, dim=-1)\n",
    "        targets = targets.detach().cpu().numpy().argmax(axis=-1)\n",
    "        outputs = outputs.detach().cpu().numpy().argmax(axis=-1)\n",
    "        scores = []\n",
    "        for output, target in zip(outputs, targets):\n",
    "            scores.append(accuracy_score(output, target))\n",
    "        train_score = np.mean(scores)\n",
    "    return train_loss, train_score\n",
    "\n",
    "############################\n",
    "# 모델 평가 함수 정의\n",
    "############################\n",
    "def test(model, loader, device, is_target=False):\n",
    "    model.eval()\n",
    "    test_pred = None\n",
    "    test_score = None\n",
    "    for enc_input, dec_input, targets in loader:\n",
    "        enc_input = enc_input.to(device)\n",
    "        dec_input = dec_input.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs, _ = model(enc_input, dec_input)\n",
    "        outputs = F.softmax(outputs, dim=-1)\n",
    "        test_pred = outputs.detach().cpu().numpy()\n",
    "        \n",
    "        if is_target:\n",
    "            targets = targets.detach().cpu().numpy().argmax(axis=-1)\n",
    "            outputs = outputs.detach().cpu().numpy().argmax(axis=-1)\n",
    "            scores = []\n",
    "            for output, target in zip(outputs, targets):\n",
    "                scores.append(accuracy_score(output, target))\n",
    "            test_score = np.mean(scores)\n",
    "    return test_pred, test_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "# 데이터\n",
    "############################\n",
    "sentences = [\n",
    "    ['ich mochte ein bier', 'i want a beer'],\n",
    "    ['guarda mi video', 'watch my video'],\n",
    "    ['hallo mein name ist kim', 'hello my name is kim'],\n",
    "    ['schau dir den mond an', 'look at the moon'],\n",
    "]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 데이터 전처리\n",
    "############################\n",
    "# 데이터 -> feature, target 분리 함수\n",
    "def data_to_Xy(datas):\n",
    "    X, y = [], []\n",
    "    for data in datas:\n",
    "        _X = data[0].split()\n",
    "        _y = data[1].split()\n",
    "        X.append(_X)\n",
    "        y.append(_y)\n",
    "    max_lan = max([len(x) for x in X+y])\n",
    "    return (X, y), max_lan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성 및 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 50382.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20, 26, 15, 12, 1], [16, 25, 7, 1, 1], [17, 24, 6, 22, 4], [29, 14, 13, 27, 9]]\n",
      "[[19, 31, 8, 11, 1], [32, 5, 7, 1, 1], [18, 5, 6, 21, 4], [23, 10, 30, 28, 1]]\n",
      "embedding type: ohe\n",
      "cell type: lstm\n",
      "--------------------model--------------------\n",
      "Ger2EngAttentionModel(\n",
      "  (_Ger2EngAttentionModel__attention): AttentionLSTM(\n",
      "    (_AttentionLSTM__encoding_layer): LSTM(128, 128)\n",
      "    (_AttentionLSTM__decoding_layer): LSTM(128, 128)\n",
      "  )\n",
      "  (_Ger2EngAttentionModel__output): OutputLinear(\n",
      "    (_OutputLinear__linear): Linear(in_features=256, out_features=33, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# 모듈 임포트\n",
    "############################\n",
    "from modules.Seq2SeqAttention import *\n",
    "\n",
    "############################\n",
    "# 데이터 전처리\n",
    "############################\n",
    "# 데이터 분리\n",
    "(X, y), max_len = data_to_Xy(sentences)\n",
    "# 임베딩 타입에 따른 데이터셋 생성\n",
    "EMBEDDING_TYPE = 'ohe' # 'ohe' or 'emb'\n",
    "ger2eng_dataset = GerEngDataset(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    max_len=max_len,\n",
    "    emb_type=EMBEDDING_TYPE,\n",
    ")\n",
    "# vocab 로드\n",
    "vocab = ger2eng_dataset.get_vocab()\n",
    "############################\n",
    "# 상수, 하이퍼파라미터 설정\n",
    "############################\n",
    "CELL_TYPE = 'lstm' # 'lstm' or 'rnn'\n",
    "BATCH_SIZE = len(X)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "HIIDEN_DIM = 128\n",
    "N_LAYERS = 1\n",
    "BIDIR = False\n",
    "LR = 0.001\n",
    "N_EPOCHS = 5000\n",
    "PATIENCE = 20\n",
    "TARGET_SCORE = 0.9\n",
    "MODEL_SAVE_PATH = '/home/parking/ml/data/models/Attn/'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CPU = 'cpu'\n",
    "\n",
    "############################\n",
    "# 기타 객체 생성\n",
    "############################\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=PATIENCE,              # 얼리스탑 횟수\n",
    "    save_path=MODEL_SAVE_PATH,      # 모델 저장 경로\n",
    "    target_score=TARGET_SCORE,      # 얼리스탑 타겟 스코어\n",
    "    model_name='Attn_model',         # 모델 이름\n",
    ")\n",
    "\n",
    "model = Ger2EngAttentionModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    emb_dim=HIIDEN_DIM,\n",
    "    hidden_dim=HIIDEN_DIM,\n",
    "    num_layers=N_LAYERS,\n",
    "    bidirectional=BIDIR,\n",
    "    emb_type=EMBEDDING_TYPE,\n",
    "    cell_type=CELL_TYPE,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f'embedding type: {EMBEDDING_TYPE}')\n",
    "print(f'cell type: {CELL_TYPE}')\n",
    "print('--------------------model--------------------')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
